{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7be03a4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ghifa\\CoDe\\ENV\\chal\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\ghifa\\CoDe\\ENV\\chal\\Lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      " * Serving Flask app '__main__' (lazy loading)\n",
      " * Environment: production\n",
      "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
      "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://127.0.0.1:5000\n",
      "Press CTRL+C to quit\n",
      "127.0.0.1 - - [04/Dec/2023 20:39:02] \"GET / HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [04/Dec/2023 20:39:02] \"GET /favicon.ico HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [04/Dec/2023 20:39:05] \"GET /docs/ HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [04/Dec/2023 20:39:05] \"GET /flasgger_static/swagger-ui.css HTTP/1.1\" 304 -\n",
      "127.0.0.1 - - [04/Dec/2023 20:39:05] \"GET /flasgger_static/swagger-ui-bundle.js HTTP/1.1\" 304 -\n",
      "127.0.0.1 - - [04/Dec/2023 20:39:05] \"GET /flasgger_static/lib/jquery.min.js HTTP/1.1\" 304 -\n",
      "127.0.0.1 - - [04/Dec/2023 20:39:05] \"GET /flasgger_static/swagger-ui-standalone-preset.js HTTP/1.1\" 304 -\n",
      "127.0.0.1 - - [04/Dec/2023 20:39:06] \"GET /flasgger_static/favicon-32x32.png HTTP/1.1\" 304 -\n",
      "127.0.0.1 - - [04/Dec/2023 20:39:06] \"GET /docs.json HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [04/Dec/2023 20:39:19] \"POST /lstm HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 737ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [04/Dec/2023 20:39:38] \"POST /lstm-file HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [04/Dec/2023 20:40:02] \"POST /nn HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [04/Dec/2023 20:40:17] \"POST /nn HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [04/Dec/2023 20:40:32] \"POST /nn-file HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "# PACKAGE\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "import pickle\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from flask import request, Flask, jsonify\n",
    "from flasgger import Swagger, LazyString, LazyJSONEncoder, swag_from\n",
    "\n",
    "# DEFINING TEXT CLEANSING\n",
    "def text_cleansing(text):\n",
    "    text = text.lower() #lowercase\n",
    "    text = text.strip() #menghapus spasi di awal dan akhir\n",
    "    text = re.sub(r'\\buser\\b|\\brt\\b|\\bamp\\b|(\\bx[\\da-f]{2})', ' ', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'\\n', ' ', text, flags=re.IGNORECASE)\n",
    "    text = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+)|(http?://[^\\s]+))',' ',text)\n",
    "    text = re.sub(r'(.)\\1\\1+', r'\\1', text) #menghapus karakter berulang\n",
    "    text = re.sub('[^0-9a-zA-Z]+', ' ', text) #menghapus karakter non-alpanumerik\n",
    "    text = re.sub(r'[øùºðµ¹ª³]', '', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'â', 'a', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip() #menghapus spasi berlebih dan mengganti dengan satu spasi\n",
    "    text = re.sub(r'^\\s+$', '', text) #menghapus seluruh kalimat yg hanya berisi spasi\n",
    "    return text\n",
    "\n",
    "# df_kamus AS A DICT\n",
    "df_kamus = pd.read_csv(\"utils/new_kamusalay.csv\", encoding=\"latin-1\", header=None)\n",
    "df_kamus_map = dict(zip(df_kamus[0], df_kamus[1]))\n",
    "def normalize(text):\n",
    "    return ' '.join([df_kamus_map[word] if word in df_kamus_map else word for word in text.split(' ')])\n",
    "\n",
    "# APPLY TEXT CLEANSING AND DICT\n",
    "def preprocess_apply(text):\n",
    "    text = text_cleansing(text)\n",
    "    text = normalize(text)\n",
    "    return text\n",
    "\n",
    "# NN\n",
    "## Load result of Feature Extraction process from NN\n",
    "file = open('resources_of_nn/feature.p','rb')\n",
    "vect = pickle.load(file)\n",
    "file.close()\n",
    "\n",
    "## Load model from NN\n",
    "nn_model = pickle.load(open('resources_of_nn/model.p', 'rb'))\n",
    "\n",
    "# DEFINE FEATURE EXTRACTION PARAMETER AND TOKENIZER CLASS\n",
    "with open ('utils/total_data', 'rb') as fp:\n",
    "    total_data = pickle.load(fp)\n",
    "\n",
    "max_features = 100000\n",
    "tokenizer = Tokenizer(num_words=max_features, split= ' ', lower=True)\n",
    "\n",
    "# DEFINE LABEL SENTIMENT\n",
    "sentiment = ['negative', 'neutral', 'positive']\n",
    "\n",
    "# LSTM\n",
    "## Load result of Feature Extraction process from LSTM\n",
    "file = open('resources_of_lstm/x_pad_sequences.pickle','rb')\n",
    "lstm_feature = pickle.load(file)\n",
    "file.close()\n",
    "\n",
    "## Load model from LSTM\n",
    "lstm_model = load_model('resources_of_lstm/model.h5')\n",
    "\n",
    "# SWAGGER UI\n",
    "app = Flask(__name__)\n",
    "\n",
    "app.json_encoder = LazyJSONEncoder\n",
    "swagger_template = dict(\n",
    "info = {\n",
    "    'title': LazyString(lambda: 'API Documentation for Machine Learning and Deep Learning'),\n",
    "    'version': LazyString(lambda: '1.0.0'),\n",
    "    'description': LazyString(lambda: 'Dokumentasi API untuk Machine Learning dan Deep Learning')\n",
    "    },\n",
    "    host = LazyString(lambda: request.host)\n",
    ")\n",
    "\n",
    "swagger_config = {\n",
    "    \"headers\": [],\n",
    "    \"specs\": [\n",
    "        {\n",
    "            \"endpoint\": 'docs',\n",
    "            \"route\": '/docs.json'\n",
    "        }\n",
    "    ],\n",
    "    \"static_url_path\": \"/flasgger_static\",\n",
    "    \"swagger_ui\": True,\n",
    "    \"specs_route\": \"/docs/\"\n",
    "}\n",
    "swagger = Swagger(app, template=swagger_template,\n",
    "                 config=swagger_config)\n",
    "\n",
    "\n",
    "# Define endpoint for Sentiment Analysis using NN\n",
    "@swag_from(r\"C:\\Users\\ghifa\\CoDe\\Platinum_Chal\\docs\\nn.yml\", methods=['POST'])\n",
    "@app.route('/nn', methods=['POST'])\n",
    "def cnn():\n",
    "    # Get text\n",
    "    original_text = request.form.get('text')\n",
    "    # Cleansing\n",
    "    text = preprocess_apply(original_text)\n",
    "    # Feature extraction\n",
    "    text_feature = vect.transform([text])\n",
    "    # Inference\n",
    "    get_sentiment = nn_model.predict(text_feature)[0]\n",
    "\n",
    "#     OUTPUT JSON RESPONSE\n",
    "    json_response = {\n",
    "        'status_code': 200,\n",
    "        'description': \"Result of Sentiment Analysis using CNN\",\n",
    "        'data': {\n",
    "            'text': original_text,\n",
    "            'sentiment': get_sentiment\n",
    "        },\n",
    "    }\n",
    "    response_data = jsonify(json_response)\n",
    "    return response_data\n",
    "\n",
    "\n",
    "# Define endpoint for Sentiment Analysis using NN from file\n",
    "@swag_from(r\"C:\\Users\\ghifa\\CoDe\\Platinum_Chal\\docs\\nn_file.yml\", methods=['POST'])\n",
    "@app.route('/nn-file', methods=['POST'])\n",
    "def nn_file():\n",
    "\n",
    "    # Upladed file\n",
    "    file = request.files.getlist('file')[0]\n",
    "    # Import file csv ke Pandas\n",
    "    df = pd.read_csv(file, encoding='latin-1')\n",
    "    # Get text from file in \"List\" format\n",
    "    texts = df.Tweet.to_list()\n",
    "    \n",
    "    # Loop list or original text and predict to model\n",
    "    text_with_sentiment = []\n",
    "    for original_text in texts:\n",
    "        # Cleansing\n",
    "        text = [preprocess_apply(original_text)]\n",
    "        # Feature extraction\n",
    "        text_feature = vect.transform(text)\n",
    "        # Inference\n",
    "        get_sentiment = nn_model.predict(text_feature)[0]\n",
    "        \n",
    "        # Predict \"text_clean\" to the Model. And insert to list \"text_with_sentiment\".\n",
    "        text_with_sentiment.append({\n",
    "            'text': original_text,\n",
    "            'sentiment': get_sentiment\n",
    "        })\n",
    "    \n",
    "#     OUTPUT JSON RESPONSE\n",
    "    json_response = {\n",
    "        'status_code': 200,\n",
    "        'description': \"Teks yang sudah diproses\",\n",
    "        'data': text_with_sentiment,\n",
    "    }\n",
    "    response_data = jsonify(json_response)\n",
    "    return response_data\n",
    "\n",
    "# Define endpoint for Sentiment Analysis using LSTM\n",
    "@swag_from(r\"C:\\Users\\ghifa\\CoDe\\Platinum_Chal\\docs\\lstm.yml\", methods=['POST'])\n",
    "@app.route('/lstm', methods=['POST'])\n",
    "def lstm():\n",
    "    # Cleansing\n",
    "    original_text = request.form.get('text')\n",
    "    text = [preprocess_apply(original_text)]\n",
    "    # Feature extraction\n",
    "    tokenizer.fit_on_texts(total_data)\n",
    "    feature = tokenizer.texts_to_sequences(text)\n",
    "    feature = pad_sequences(feature, maxlen=lstm_feature.shape[1])\n",
    "    # Inference\n",
    "    prediction = lstm_model(feature)\n",
    "    get_sentiment = sentiment[np.argmax(prediction[0])]\n",
    "\n",
    "#     OUTPUT JSON RESPONSE\n",
    "    json_response = {\n",
    "        'status_code': 200,\n",
    "        'description': \"Hasil Teks dan Sentimennya\",\n",
    "        'data': {\n",
    "            'text': original_text,\n",
    "            'sentiment': get_sentiment\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    response_data = jsonify(json_response)\n",
    "    return response_data\n",
    "\n",
    "# Define endpoint for Sentiment Analysis using LSTM from file\n",
    "@swag_from(r\"C:\\Users\\ghifa\\CoDe\\Platinum_Chal\\docs\\lstm_file.yml\", methods=['POST'])\n",
    "@app.route('/lstm-file', methods=['POST'])\n",
    "def lstm_file():\n",
    "    \n",
    "#     Upladed file and import\n",
    "    filein = request.files.getlist('filein')[0]\n",
    "    df = pd.read_csv(filein,encoding=\"latin-1\")\n",
    "#     pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "#     Get text from file in \"List\" format\n",
    "    texts = df.Tweet.to_list()\n",
    "    \n",
    "    tokenizer.fit_on_texts(total_data)\n",
    "\n",
    "#     Loop list or original text and predict to model\n",
    "    text_with_sentiment = []\n",
    "    for original_text in texts:\n",
    "        # Cleansing\n",
    "        text = [preprocess_apply(original_text)]\n",
    "        # Feature extraction\n",
    "        feature = tokenizer.texts_to_sequences(text)\n",
    "        feature = pad_sequences(feature, maxlen=lstm_feature.shape[1])\n",
    "        # Inference\n",
    "        prediction = lstm_model.predict(feature)\n",
    "        get_sentiment = sentiment[np.argmax(prediction[0])]\n",
    "\n",
    "        # Predict \"text_clean\" to the Model. And insert to list \"text_with_sentiment\".\n",
    "        text_with_sentiment.append({\n",
    "            'text': original_text,\n",
    "            'sentiment': get_sentiment\n",
    "        })\n",
    "    \n",
    "#     OUTPUT JSON RESPONSE\n",
    "    json_response = {\n",
    "        'status_code': 200,\n",
    "        'description': \"File yang sudah diproses\",\n",
    "        'data': text_with_sentiment\n",
    "    }\n",
    "    \n",
    "    response_data = jsonify(json_response)\n",
    "    return response_data\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chal",
   "language": "python",
   "name": "chal"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
